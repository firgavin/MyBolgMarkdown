---
title: DataMining_DecisionTree_ID3
grammar_cjkRuby: true
---
# DecisionTree算法实现
## 决策树简介
首先决策树属于分类算法。**分类**问题可以分为两类：

> 归类：归类是指对离散数据的分类，比如对根据一个人的笔迹判别这个是男还是女，这里的类别只有两个，类别是离散的集合空间{男，女}的。
> 
> 预测：预测是指对连续数据的分类，比如预测明天8点天气的湿度情况，天气的湿度在随时变化，8点时的天气是一个具体值，它不属于某个有限集合空间。预测也叫回归分析，在金融领域有着广泛应用。

虽然对离散数据和连续数据的处理方式有所不同，但其实他们之间相互转化，比如我们可以根据比较的某个特征值判断。决策树是用样本的**属性作为结点，用属性的取值作为分支**的树结构。 
决策树的根结点是所有样本中信息量最大的属性。树的中间结点是该结点为根的子树所包含的样本子集中**信息量最大的属性**。决策树的叶结点是样本的类别值。决策树是一种知识表示形式，它是对所有样本数据的高度概括决策树能准确地识别所有样本的类别，也能有效地识别新样本的类别。

> **DecisionTree基本思想**

```
算法：GenerateDecisionTree(D,attributeList)根据训练数据记录D生成一棵决策树.
输入：
	数据记录D，包含类标的训练数据集;
	属性列表attributeList，候选属性集，用于在内部结点中作判断的属性.
	属性选择方法AttributeSelectionMethod()，选择最佳分类属性的方法.
输出：一棵决策树.
过程：
	1、构造一个节点N；
	2、如果数据记录D中的所有记录的类标都相同（记为C类）：则将节点N作为叶子节点标记为C，并返回结点N；
	3、如果属性列表为空：则将节点N作为叶子结点标记为D中类标最多的类，并返回结点N；
	4、调用AttributeSelectionMethod(D,attributeList)选择最佳的分裂准则splitCriterion;
	5、将节点N标记为最佳分裂准则splitCriterion;
	6、如果分裂属性取值是离散的，并且允许决策树进行多叉分裂：从属性列表中减去分裂属性，attributeLsit -= splitAttribute;
	7、对分裂属性的每一个取值j:记D中满足j的记录集合为Dj;如果Dj为空：则新建一个叶子结点F，标记为D中类标最多的类，并且把结点F挂在N下;
	8、否则：递归调用GenerateDecisionTree(Dj,attributeList)得到子树结点Nj，将Nj挂在N下;
	9、返回结点N;
```

> 算法的时间复杂度是**O(k*|D|*log(|D|))**，k为属性个数，|D|为记录集D的记录数。

## DecisionTree的实现问题
### 分裂准则对应元祖划分的可能性
根据分裂准则划分元组共有三种可能性，设A是分裂属性：

> A是离散值：对A的每个已知值产生一个分枝。
> A是连续值：产生两个分枝，选定split_point，分别对应A<=split_point和A>split_point。
> A是离散值且必须要产生二叉树：测试形如A属于S，其中S是A的分裂子集。

### 属性选择度量
#### 信息增益
<img src="http://chart.googleapis.com/chart?cht=tx&chl= Info(D)=-\sum_{i=1}^{m}p_{i}log_{2}(p_{i})" style="border:none;">
<br>
<img src="http://chart.googleapis.com/chart?cht=tx&chl= Info_{A}(D)=\sum^{v}_{j=1}\frac{ \left | D_{j} \right |}{ \left | D \right |}\times Info(D_{j})" style="border:none;">
<br>
<img src="http://chart.googleapis.com/chart?cht=tx&chl= Gain(A)=Info(D)-Info_A(D)" style="border:none;">

#### 增益率


#### 基尼指数

### 树剪枝
### 可伸缩性与决策树归纳

## DecisionTree_ID3实现（Python）
``` python
# -*- coding: utf-8 -*-
from numpy import *
import numpy as np
from math import log
import operator
import matplotlib.pyplot as plt
decisionNode=dict(boxstyle="sawtooth",fc="0.8")
leafNode=dict(boxstyle="round4",fc="0.8")
arrow_args=dict(arrowstyle="<-")

#计算数据集的香农熵
def calcShannonEnt(dataSet):
    #calculate the shannon value
    numEntries = len(dataSet)
    labelCounts = {}
    for featVec in dataSet:      #create the dictionary for all of the data
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob*log(prob,2) #get the log value
    return shannonEnt

#创建数据的函数
def createDataSet():
    dataSet = [[1,1,'yes'],
               [1,1, 'yes'],
               [1,0,'no'],
               [0,1,'no'],
               [0,1,'no']]
    labels = ['no surfacing','flippers']
    return dataSet, labels

#划分数据集
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:      #abstract the fature
            reducedFeatVec = featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet

#选择最好的数据集划分方式
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0])-1
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain = 0.0; bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i , value)
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy +=prob * calcShannonEnt(subDataSet)
        infoGain = baseEntropy - newEntropy
        if(infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature

def majorityCnt(classList):
    classCount = {}
    for vote in classList:
        if vote not in classCount.keys(): classCount[vote] = 0
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]

def createTree(dataSet, labels):
    classList = [example[-1] for example in dataSet]
    # the type is the same, so stop classify
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # traversal all the features and choose the most frequent feature
    if (len(dataSet[0]) == 1):
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel:{}}
    del(labels[bestFeat])
    #get the list which attain the whole properties
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    for value in uniqueVals:
        subLabels = labels[:]
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)
    return myTree

def classify(inputTree, featLabels, testVec):
    firstStr = inputTree.keys()[0]
    secondDict = inputTree[firstStr]
    featIndex = featLabels.index(firstStr)
    for key in secondDict.keys():
        if testVec[featIndex] == key:
            if type(secondDict[key]).__name__ == 'dict':
                classLabel = classify(secondDict[key], featLabels, testVec)
            else: classLabel = secondDict[key]
    return classLabel


# 计算树的叶子节点数量
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            numLeafs += getNumLeafs(secondDict[key])
        else:
            numLeafs += 1
    return numLeafs


# 计算树的最大深度
def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:
            thisDepth = 1
        if thisDepth > maxDepth:
            maxDepth = thisDepth
    return maxDepth


# 画节点
def plotNode(nodeTxt, centerPt, parentPt, nodeType):
    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction', \
                            xytext=centerPt, textcoords='axes fraction', va="center", ha="center", \
                            bbox=nodeType, arrowprops=arrow_args)


# 画箭头上的文字
def plotMidText(cntrPt, parentPt, txtString):
    lens = len(txtString)
    xMid = (parentPt[0] + cntrPt[0]) / 2.0 - lens * 0.002
    yMid = (parentPt[1] + cntrPt[1]) / 2.0
    createPlot.ax1.text(xMid, yMid, txtString)


def plotTree(myTree, parentPt, nodeTxt):
    numLeafs = getNumLeafs(myTree)
    depth = getTreeDepth(myTree)
    firstStr = myTree.keys()[0]
    cntrPt = (plotTree.x0ff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.y0ff)
    plotMidText(cntrPt, parentPt, nodeTxt)
    plotNode(firstStr, cntrPt, parentPt, decisionNode)
    secondDict = myTree[firstStr]
    plotTree.y0ff = plotTree.y0ff - 1.0 / plotTree.totalD
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            plotTree(secondDict[key], cntrPt, str(key))
        else:
            plotTree.x0ff = plotTree.x0ff + 1.0 / plotTree.totalW
            plotNode(secondDict[key], (plotTree.x0ff, plotTree.y0ff), cntrPt, leafNode)
            plotMidText((plotTree.x0ff, plotTree.y0ff), cntrPt, str(key))
    plotTree.y0ff = plotTree.y0ff + 1.0 / plotTree.totalD


def createPlot(inTree):
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)
    plotTree.totalW = float(getNumLeafs(inTree))
    plotTree.totalD = float(getTreeDepth(inTree))
    plotTree.x0ff = -0.5 / plotTree.totalW
    plotTree.y0ff = 1.0
    plotTree(inTree, (0.5, 1.0), '')
    plt.show()

myDat, labels = createDataSet()
myTree = createTree(myDat,labels)
print myTree

createPlot(myTree)

myDat, labels = createDataSet()
classificaion=classify(myTree,labels,[1,0])
print classificaion
```
